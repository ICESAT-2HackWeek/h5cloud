{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82cf477-8ba2-4783-8726-18b784e51368",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# ICESat-2 HDF5 Cloud Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742be131-1ef0-433d-a0cd-0d8c8ffc2a4a",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Experiment Setup\n",
    "\n",
    "Subset data in 3 ATL03 granules stored in icesat2-cloud-experiments bucket.\n",
    "\n",
    "* User provides a bounding box.\n",
    "* User gets a numpy array with data from the `/gtx/heights` group, specifically: delta_time, ds_ph_along, lat_ph, lon_ph and h_ph from all 6 tracks\n",
    "\n",
    "Data dictionary: https://nsidc.org/sites/default/files/documents/technical-reference/icesat2_atl03_data_dict_v006.pdf\n",
    "\n",
    "Assumptions:\n",
    "* We don't consider Earthdata login so we put data in a bucket where we can circumvent EDL and credentials\n",
    "* Granules is in an S3 bucket in us-west-2, like the cryocloud hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd4392-4eb7-402a-92bf-fc8a1bb484f6",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Baseline: Subsetting via direct file access using s3fs + h5py\n",
    "\n",
    "With ATL03, the filesize may be too large for this to be a reasonable experiment. Should we benchmark file s3fs --> subsetting via h5py???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ff6a5-eb9c-4db8-8acc-6455c88eeec2",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Option 1: Subset data through a server (Sliderule) \n",
    "\n",
    "Sliderule uses h5coro C++ implementation under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67af99-8b53-4488-a81b-8f8b4c473e3a",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Option 2: Subset data through a custom h5 library which optimizes access (h5coro)\n",
    "\n",
    "h5coro may be considerd reads the metadata on the fly and then threads share a cache to optimize subsetting operations.\n",
    "\n",
    "60ms has been stated as the initial overhead in reading data from S3. H5Coro optimizes reading chunks of data from S3 by \"overreading\" and then the threads performing subsetting operations access a shared cache from that overread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce24902-2f5a-4a6f-abfb-cbe24bf6c183",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Option 3: Kerchunk\n",
    "\n",
    "A kerchunk reference will be created from the 3 granules. A kerchunk reference can be used by xarray to \"lazily load\" the file metadata and then subset by reading only the chunks requested by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
